# RAG-Based Document Question Answering System

A **Retrieval-Augmented Generation (RAG)** based Document Question Answering system that enables users to ask natural language questions over documents and receive accurate, context-aware answers using **semantic search** and **Large Language Models (LLMs)**.

This project demonstrates a **real-world RAG pipeline** combining document ingestion, vector similarity search, and LLM-powered answer generation, suitable for AI-powered search engines and enterprise knowledge assistants.

---

## ğŸ“Œ Overview

Traditional LLMs may hallucinate or provide outdated information.  
This system solves that by **retrieving relevant document context first** and then generating answers grounded in actual data.

**Key idea:**  
> *Retrieve first â†’ Generate second*

---

## ğŸš€ Key Features

- ğŸ“„ Document ingestion and preprocessing
- ğŸ” Semantic search using vector embeddings
- ğŸ§  Retrieval-Augmented Generation (RAG) architecture
- ğŸ¤– LLM-powered answer generation
- ğŸ“Š JSON-based input/output handling
- ğŸ“ Logging for debugging and traceability
- ğŸ§© Modular and extensible Python codebase
- âš™ï¸ Shell-based deployment support

---

## ğŸ—ï¸ Architecture

```

User Question
â†“
Vector Embedding
â†“
Semantic Retriever
â†“
Relevant Document Context
â†“
LLM (GPT-based)
â†“
Final Answer

```

---

## ğŸ§° Tech Stack

- **Language:** Python
- **LLM Integration:** GPT-based client
- **Vectorization:** Embedding-based semantic search
- **Data Formats:** JSON
- **Deployment:** Shell scripting
- **Version Control:** Git & GitHub

---

## ğŸ“‚ Project Structure

```

rag-based-document-qa/
â”‚
â”œâ”€â”€ app.py                 # Application entry point
â”œâ”€â”€ main.py                # Core execution flow
â”œâ”€â”€ document_loader.py     # Document ingestion & preprocessing
â”œâ”€â”€ vectorizer.py          # Embedding generation logic
â”œâ”€â”€ retriever.py           # Semantic similarity search
â”œâ”€â”€ gpt_client.py          # LLM interaction
â”œâ”€â”€ submitter.py           # Output handling
â”œâ”€â”€ deploy.sh              # Deployment helper script
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ questions.json         # Sample input questions
â”œâ”€â”€ answers_output.json    # Generated answers
â”œâ”€â”€ log.txt                # Execution logs
â””â”€â”€ README.md

````

---

## â–¶ï¸ How to Run the Project

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/Debasish-87/rag-based-document-qa.git
cd rag-based-document-qa
````

### 2ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ Configure LLM Access

* Add your API key inside `gpt_client.py`
* Or configure environment variables as required

### 4ï¸âƒ£ Run the Application

```bash
python app.py
```

### 5ï¸âƒ£ View Results

* Generated answers â†’ `answers_output.json`
* Logs â†’ `log.txt`

---

## ğŸ“Œ Sample Workflow

1. Load documents
2. Chunk and vectorize content
3. Accept user questions from JSON
4. Retrieve relevant context using semantic similarity
5. Generate grounded answers via LLM
6. Store output for analysis

---

## ğŸ§ª Use Cases

* ğŸ“š Enterprise document Q&A
* ğŸ” AI-powered knowledge search
* ğŸ“„ Research paper analysis
* ğŸ¢ Internal documentation assistants
* ğŸ¤– Chatbots with document grounding

---

## ğŸ” Security & Limitations

* API keys must be kept secure (never commit secrets)
* Designed for learning and demonstration purposes
* Can be extended with production-grade vector databases

---

## ğŸš§ Future Enhancements

* FAISS / Pinecone / ChromaDB integration
* Web-based UI (FastAPI / Streamlit)
* Support for PDF, DOCX, and HTML documents
* Authentication & access control
* Performance optimization for large datasets

---

## ğŸ‘¨â€ğŸ’» Contributors

* **Debasish Mohanty** â€“ Core development & architecture
* **Rudra Prasad Jena** â€“ Collaboration & contributions
* **Srujan Rana** â€“ Feature enhancements

---

## ğŸ“œ License

This project is licensed under the **MIT License**.

---

